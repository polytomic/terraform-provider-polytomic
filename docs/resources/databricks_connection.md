---
# generated by https://github.com/fbreckle/terraform-plugin-docs
page_title: "polytomic_databricks_connection Resource - terraform-provider-polytomic"
subcategory: "Connections"
description: |-
  Databricks Connection
---

# polytomic_databricks_connection (Resource)

Databricks Connection

## Example Usage

```terraform
resource "polytomic_databricks_connection" "databricks" {
  name = "example"
  configuration = {
    access_token          = "isoz8af6zvp8067gu68gvrp0oftevn"
    auth_mode             = "access_key_and_secret"
    aws_access_key_id     = "AKIAIOSFODNN7EXAMPLE"
    aws_secret_access_key = "wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY"
    azure_access_key      = "abcdefghijklmnopqrstuvwxyz0123456789/+ABCDEabcdefghijklmnopqrstuvwxyz0123456789/+ABCDE=="
    azure_account_name    = "account"
    cloud_provider        = "aws"
    container_name        = "container"
    http_path             = "/sql"
    s3_bucket_name        = "s3://polytomic-databricks-results/customer-dataset"
    server_hostname       = "dbc-1234dsafas-d0001.cloud.databricks.com"
  }
}
```

<!-- schema generated by tfplugindocs -->
## Schema

### Required

- `configuration` (Attributes) (see [below for nested schema](#nestedatt--configuration))
- `name` (String)

### Optional

- `force_destroy` (Boolean) Indicates whether dependent models, syncs, and bulk syncs should be cascade deleted when this connection is destroy. This only deletes other resources when the connection is destroyed, not when setting this parameter to `true`. Once this parameter is set to `true`, there must be a successful `terraform apply` run before a destroy is required to update this value in the resource state. Without a successful `terraform apply` after this parameter is set, this flag will have no effect. If setting this field in the same operation that would require replacing the connection or destroying the connection, this flag will not work. Additionally when importing a connection, a successful `terraform apply` is required to set this value in state before it will take effect on a destroy operation.
- `organization` (String) Organization ID

### Read-Only

- `id` (String) Databricks Connection identifier

<a id="nestedatt--configuration"></a>
### Nested Schema for `configuration`

Required:

- `access_token` (String, Sensitive)
- `auth_mode` (String) How to authenticate with AWS. Defaults to Access Key and Secret
- `http_path` (String)
- `port` (Number)
- `server_hostname` (String)

Optional:

- `aws_access_key_id` (String) See https://docs.polytomic.com/docs/databricks-connections#writing-to-databricks
- `aws_secret_access_key` (String, Sensitive)
- `aws_user` (String)
- `azure_access_key` (String, Sensitive) The access key associated with this storage account
- `azure_account_name` (String) The account name of the storage account
- `cloud_provider` (String)
- `concurrent_queries` (Number)
- `container_name` (String) The container which we will stage files in
- `deleted_file_retention_days` (Number)
- `enable_delta_uniform` (Boolean)
- `enforce_query_limit` (Boolean)
- `external_id` (String) External ID for the IAM role
- `iam_role_arn` (String)
- `log_file_retention_days` (Number)
- `s3_bucket_name` (String) Name of bucket used for staging data load files
- `s3_bucket_region` (String) Region of bucket.example=us-east-1
- `set_retention_properties` (Boolean)
- `storage_credential_name` (String)
- `unity_catalog_enabled` (Boolean)


